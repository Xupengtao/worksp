{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-a670acca0a94>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/shuzhilian/anaconda3/envs/mask_rcnn/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/shuzhilian/anaconda3/envs/mask_rcnn/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/shuzhilian/anaconda3/envs/mask_rcnn/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/shuzhilian/anaconda3/envs/mask_rcnn/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/shuzhilian/anaconda3/envs/mask_rcnn/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 指定使用GPU编号\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "#import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义基本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义池化层\n",
    "def max_pool(x, size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, size, size, 1], strides=[1, size, size, 1], padding='SAME')\n",
    "\n",
    "# 定义卷积操作\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 定义权重\n",
    "def weight_varibale(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 定义偏置\n",
    "def bias_varibale(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 定义基本卷积层\n",
    "def conv_block(input_, kernel_size=3, input_channel=1, output_channel=32, activate=True):\n",
    "    # 定义\n",
    "    w_conv = weight_varibale([kernel_size, kernel_size, input_channel, output_channel])\n",
    "    b_conv = weight_varibale([output_channel])\n",
    "    if activate == True:                              \n",
    "        conv_out = tf.nn.relu(conv2d(input_, w_conv) + b_conv)\n",
    "    else:\n",
    "        conv_out = conv2d(input_, w_conv0) + b_conv0\n",
    "    return conv_out\n",
    "\n",
    "# 定义全连接层\n",
    "def fc_block(input_, input_dim=1, outout_dim=32, activate='relu'):\n",
    "    w_fc = weight_varibale([input_dim, outout_dim])\n",
    "    b_fc = weight_varibale([outout_dim])\n",
    "    if activate == 'relu':                              \n",
    "        fc_out = tf.nn.relu(tf.matmul(input_, w_fc) + b_fc)\n",
    "    elif activate == 'sigmoid':  \n",
    "        fc_out = tf.nn.sigmoid(tf.matmul(input_, w_fc) + b_fc)\n",
    "    else:\n",
    "        fc_out = tf.matmul(input_, w_fc) + b_fc\n",
    "    return fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "IMG_CHANNEL = 1\n",
    "IMG_CLASS = 10\n",
    "TRAIN_RATE = 0.001\n",
    "TRAIN_RATE_DELAY = 0.99\n",
    "BATCH_SIZE = 64\n",
    "ITER = 3000    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入输出占位符\n",
    "X = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, IMG_CHANNEL])\n",
    "y_ = tf.placeholder(tf.float32, [None, IMG_CLASS])\n",
    "\n",
    "# 记录输入图片\n",
    "tf.summary.image('input', X, 3)\n",
    "\n",
    "# 第1层卷积层\n",
    "conv_1 = conv_block(input_=X, kernel_size=3, input_channel=IMG_CHANNEL, output_channel=16, activate=True)\n",
    "\n",
    "# 第2层卷积层\n",
    "conv_2 = conv_block(input_=conv_1, kernel_size=3, input_channel=16, output_channel=32, activate=True)\n",
    "\n",
    "# 池化层\n",
    "conv_2 = max_pool(conv_2, 2)\n",
    "\n",
    "# 第3层卷积层\n",
    "conv_3 = conv_block(input_=conv_2, kernel_size=3, input_channel=32, output_channel=32, activate=True)\n",
    "\n",
    "# 第4层卷积层\n",
    "conv_4 = conv_block(input_=conv_3, kernel_size=3, input_channel=32, output_channel=32, activate=True)\n",
    "\n",
    "# 池化层\n",
    "conv_4 = max_pool(conv_4, 2)\n",
    "\n",
    "# 将三维特征拉成一列\n",
    "conv_4_shape = conv_4.get_shape().as_list()\n",
    "conv_4_len = conv_4_shape[1] * conv_4_shape[2] * conv_4_shape[3]\n",
    "conv_4_reshape = tf.reshape(conv_4, shape=[-1, conv_4_len])\n",
    "\n",
    "# 全连接层\n",
    "fc_1 = fc_block(input_=conv_4_reshape, input_dim=conv_4_len, outout_dim=128, activate='relu')\n",
    "\n",
    "# 输出层\n",
    "fc_out = fc_block(input_=fc_1, input_dim=128, outout_dim=10, activate='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 28, 28, 1), dtype=float32) Tensor(\"Sigmoid:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X, fc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数和准确性度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y_, logits=fc_out))\n",
    "# 记录损失函数\n",
    "tf.summary.scalar('loss', cross_entropy)\n",
    "\n",
    "# 准确率度量\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(fc_out, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "# 记录准确率\n",
    "tf.summary.scalar('acc', acc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "learn_rate = tf.train.exponential_decay(TRAIN_RATE, global_step, 10, TRAIN_RATE_DELAY)\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy, global_step=global_step)\n",
    "tf.summary.scalar('learn_rate', learn_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记录训练时的参数和创建保存模型的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('SummarWriter/' + 'train', tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter('SummarWriter/' + 'test', tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 train_acc: 0.09375 train_loss 2.2903957\n",
      "iter: 0 test_acc: 0.140625 test_loss 2.2956057\n",
      "\n",
      "\n",
      "iter: 100 train_acc: 0.921875 train_loss 1.5856702\n",
      "iter: 100 test_acc: 0.921875 test_loss 1.541276\n",
      "\n",
      "\n",
      "iter: 200 train_acc: 0.984375 train_loss 1.493273\n",
      "iter: 200 test_acc: 0.96875 test_loss 1.5213857\n",
      "\n",
      "\n",
      "iter: 300 train_acc: 0.921875 train_loss 1.5051115\n",
      "iter: 300 test_acc: 0.953125 test_loss 1.4977945\n",
      "\n",
      "\n",
      "iter: 400 train_acc: 0.921875 train_loss 1.5140972\n",
      "iter: 400 test_acc: 0.96875 test_loss 1.4949093\n",
      "\n",
      "\n",
      "iter: 500 train_acc: 0.953125 train_loss 1.4918191\n",
      "iter: 500 test_acc: 0.953125 test_loss 1.5033385\n",
      "\n",
      "\n",
      "iter: 600 train_acc: 0.953125 train_loss 1.5021191\n",
      "iter: 600 test_acc: 0.984375 test_loss 1.4860845\n",
      "\n",
      "\n",
      "iter: 700 train_acc: 0.984375 train_loss 1.4859986\n",
      "iter: 700 test_acc: 0.96875 test_loss 1.487331\n",
      "\n",
      "\n",
      "iter: 800 train_acc: 0.953125 train_loss 1.4989743\n",
      "iter: 800 test_acc: 0.953125 test_loss 1.4897635\n",
      "\n",
      "\n",
      "iter: 900 train_acc: 0.984375 train_loss 1.4877546\n",
      "iter: 900 test_acc: 0.96875 test_loss 1.4751589\n",
      "\n",
      "\n",
      "iter: 1000 train_acc: 1.0 train_loss 1.471095\n",
      "iter: 1000 test_acc: 1.0 test_loss 1.4709177\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建会话\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(ITER):\n",
    "        train_x, train_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        train_x = sess.run(tf.reshape(train_x, (BATCH_SIZE, IMG_SIZE, IMG_SIZE, IMG_CHANNEL)))\n",
    "        test_x, test_y = mnist.validation.next_batch(BATCH_SIZE)\n",
    "        test_x = sess.run(tf.reshape(test_x, (BATCH_SIZE, IMG_SIZE, IMG_SIZE, IMG_CHANNEL)))\n",
    "    \n",
    "        _ = sess.run(train_step, feed_dict={X:train_x, y_:train_y})\n",
    "        \n",
    "        summary = sess.run(merge, feed_dict={X:train_x, y_:train_y})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "        summary = sess.run(merge, feed_dict={X:test_x, y_:test_y})\n",
    "        test_writer.add_summary(summary, i)\n",
    "        \n",
    "        \n",
    "        if i % 100 == 0:            \n",
    "            train_acc, train_loss = sess.run([acc, cross_entropy], feed_dict={X:train_x, y_:train_y})\n",
    "            print('iter:', i, 'train_acc:', train_acc, 'train_loss', train_loss)\n",
    "            \n",
    "            test_acc, test_loss = sess.run([acc, cross_entropy], feed_dict={X:test_x, y_:test_y})\n",
    "            print('iter:', i, 'test_acc:', test_acc, 'test_loss', test_loss)\n",
    "            print('\\n')\n",
    "            \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess, './ckpt/' + 'mycnn' + '-' + str(i) + '/' + 'mycnn', global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
